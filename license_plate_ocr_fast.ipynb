{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c80a4ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petitoff/repos/license_plate_ocr/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[33mChecking connectivity to the model hosters, this may take a while. To bypass this check, set `DISABLE_MODEL_SOURCE_CHECK` to `True`.\u001b[0m\n",
      "/tmp/ipykernel_106729/1864221780.py:15: UserWarning: `lang` and `ocr_version` will be ignored when model names or model directories are not `None`.\n",
      "  ocr = PaddleOCR(\n",
      "/home/petitoff/repos/license_plate_ocr/.venv/lib/python3.12/site-packages/paddle/utils/cpp_extension/extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/petitoff/.paddlex/official_models/PP-LCNet_x1_0_textline_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv4_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/petitoff/.paddlex/official_models/PP-OCRv4_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv4_server_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `/home/petitoff/.paddlex/official_models/PP-OCRv4_server_rec`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path: /home/petitoff/repos/license_plate_ocr/z29927594AMP.jpg\n",
      "model_path: /home/petitoff/repos/license_plate_ocr/license-plate-finetune-v1n.onnx\n",
      "Model input shape: ['batch', 3, 'height', 'width'] (dynamic dims will use 640)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from paddleocr import PaddleOCR\n",
    "\n",
    "# --- 1. Setup ---\n",
    "# Initialize PaddleOCR (downloads model automatically on first run)\n",
    "# lang='en' covers english alphanumeric characters (standard for plates)\n",
    "# Użyj modelu \"server\" zamiast \"mobile\" - wolniejszy ale dokładniejszy\n",
    "# det_model_name - model detekcji tekstu\n",
    "# rec_model_name - model rozpoznawania tekstu (to jest kluczowe!)\n",
    "ocr = PaddleOCR(\n",
    "    lang=\"en\",\n",
    "    ocr_version=\"PP-OCRv4\",\n",
    "    text_detection_model_name=\"PP-OCRv4_server_det\",\n",
    "    text_recognition_model_name=\"PP-OCRv4_server_rec\",\n",
    "    use_doc_orientation_classify=False,\n",
    "    use_doc_unwarping=False,\n",
    "    use_textline_orientation=True,  # odpowiednik \"angle/cls\" w nowym pipeline\n",
    ")\n",
    "\n",
    "root = Path.cwd()\n",
    "image_path = root / \"z29927594AMP.jpg\"\n",
    "model_path = root / \"license-plate-finetune-v1n.onnx\"  # Use ONNX model\n",
    "\n",
    "assert image_path.exists(), f\"Missing image: {image_path}\"\n",
    "assert model_path.exists(), f\"Missing model: {model_path}\"\n",
    "\n",
    "# Load ONNX model with onnxruntime\n",
    "session = ort.InferenceSession(str(model_path), providers=['CPUExecutionProvider'])\n",
    "input_name = session.get_inputs()[0].name\n",
    "input_shape = session.get_inputs()[0].shape  # e.g., [1, 3, 640, 640] or dynamic ['batch', 3, 'height', 'width']\n",
    "\n",
    "print(\"image_path:\", image_path)\n",
    "print(\"model_path:\", model_path)\n",
    "print(f\"Model input shape: {input_shape} (dynamic dims will use 640)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "410cb7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX YOLO helpers loaded!\n"
     ]
    }
   ],
   "source": [
    "# --- ONNX YOLO Inference Helpers (no ultralytics needed) ---\n",
    "\n",
    "def preprocess_image_for_yolo(image_bgr: np.ndarray, target_size: tuple[int, int] = (640, 640)) -> tuple[np.ndarray, tuple[float, float], tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Preprocess image for YOLOv8 ONNX model.\n",
    "    Returns: (preprocessed_tensor, scale_factors, padding)\n",
    "    \"\"\"\n",
    "    h, w = image_bgr.shape[:2]\n",
    "    target_h, target_w = target_size\n",
    "    \n",
    "    # Calculate scale to fit while maintaining aspect ratio\n",
    "    scale = min(target_w / w, target_h / h)\n",
    "    new_w, new_h = int(w * scale), int(h * scale)\n",
    "    \n",
    "    # Resize image\n",
    "    resized = cv2.resize(image_bgr, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Create padded image (letterbox)\n",
    "    padded = np.full((target_h, target_w, 3), 114, dtype=np.uint8)\n",
    "    pad_w = (target_w - new_w) // 2\n",
    "    pad_h = (target_h - new_h) // 2\n",
    "    padded[pad_h:pad_h + new_h, pad_w:pad_w + new_w] = resized\n",
    "    \n",
    "    # Convert BGR -> RGB, HWC -> CHW, normalize to [0, 1]\n",
    "    img_rgb = cv2.cvtColor(padded, cv2.COLOR_BGR2RGB)\n",
    "    img_chw = img_rgb.transpose(2, 0, 1).astype(np.float32) / 255.0\n",
    "    img_batch = np.expand_dims(img_chw, axis=0)  # Add batch dimension\n",
    "    \n",
    "    return img_batch, (scale, scale), (pad_w, pad_h)\n",
    "\n",
    "\n",
    "def postprocess_yolo_output(\n",
    "    output: np.ndarray, \n",
    "    original_shape: tuple[int, int],\n",
    "    scale: tuple[float, float],\n",
    "    padding: tuple[int, int],\n",
    "    conf_threshold: float = 0.25,\n",
    "    iou_threshold: float = 0.45\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Postprocess YOLOv8 ONNX output to get bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        output: Model output of shape [1, num_classes+4, num_predictions] (YOLOv8 format)\n",
    "        original_shape: Original image (h, w)\n",
    "        scale: Scale factors used during preprocessing\n",
    "        padding: Padding (pad_w, pad_h) used during preprocessing\n",
    "        conf_threshold: Confidence threshold\n",
    "        iou_threshold: IoU threshold for NMS\n",
    "    \n",
    "    Returns:\n",
    "        List of detections with 'bbox_xyxy' and 'conf'\n",
    "    \"\"\"\n",
    "    # YOLOv8 output is [1, 4+num_classes, num_predictions]\n",
    "    # Transpose to [num_predictions, 4+num_classes]\n",
    "    predictions = output[0].T  # Shape: [num_predictions, 4+num_classes]\n",
    "    \n",
    "    # Extract boxes (cx, cy, w, h) and class scores\n",
    "    boxes_cxcywh = predictions[:, :4]\n",
    "    class_scores = predictions[:, 4:]\n",
    "    \n",
    "    # Get best class and confidence for each prediction\n",
    "    confidences = np.max(class_scores, axis=1)\n",
    "    class_ids = np.argmax(class_scores, axis=1)\n",
    "    \n",
    "    # Filter by confidence\n",
    "    mask = confidences > conf_threshold\n",
    "    boxes_cxcywh = boxes_cxcywh[mask]\n",
    "    confidences = confidences[mask]\n",
    "    class_ids = class_ids[mask]\n",
    "    \n",
    "    if len(boxes_cxcywh) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Convert cx, cy, w, h -> x1, y1, x2, y2\n",
    "    boxes_xyxy = np.zeros_like(boxes_cxcywh)\n",
    "    boxes_xyxy[:, 0] = boxes_cxcywh[:, 0] - boxes_cxcywh[:, 2] / 2  # x1\n",
    "    boxes_xyxy[:, 1] = boxes_cxcywh[:, 1] - boxes_cxcywh[:, 3] / 2  # y1\n",
    "    boxes_xyxy[:, 2] = boxes_cxcywh[:, 0] + boxes_cxcywh[:, 2] / 2  # x2\n",
    "    boxes_xyxy[:, 3] = boxes_cxcywh[:, 1] + boxes_cxcywh[:, 3] / 2  # y2\n",
    "    \n",
    "    # Remove padding and rescale to original image coordinates\n",
    "    pad_w, pad_h = padding\n",
    "    scale_x, scale_y = scale\n",
    "    \n",
    "    boxes_xyxy[:, 0] = (boxes_xyxy[:, 0] - pad_w) / scale_x\n",
    "    boxes_xyxy[:, 1] = (boxes_xyxy[:, 1] - pad_h) / scale_y\n",
    "    boxes_xyxy[:, 2] = (boxes_xyxy[:, 2] - pad_w) / scale_x\n",
    "    boxes_xyxy[:, 3] = (boxes_xyxy[:, 3] - pad_h) / scale_y\n",
    "    \n",
    "    # Clip to image bounds\n",
    "    orig_h, orig_w = original_shape\n",
    "    boxes_xyxy[:, 0] = np.clip(boxes_xyxy[:, 0], 0, orig_w)\n",
    "    boxes_xyxy[:, 1] = np.clip(boxes_xyxy[:, 1], 0, orig_h)\n",
    "    boxes_xyxy[:, 2] = np.clip(boxes_xyxy[:, 2], 0, orig_w)\n",
    "    boxes_xyxy[:, 3] = np.clip(boxes_xyxy[:, 3], 0, orig_h)\n",
    "    \n",
    "    # Apply NMS\n",
    "    indices = cv2.dnn.NMSBoxes(\n",
    "        boxes_xyxy.tolist(),\n",
    "        confidences.tolist(),\n",
    "        conf_threshold,\n",
    "        iou_threshold\n",
    "    )\n",
    "    \n",
    "    # Build results\n",
    "    results = []\n",
    "    for i in indices:\n",
    "        idx = i[0] if isinstance(i, (list, np.ndarray)) else i\n",
    "        results.append({\n",
    "            'bbox_xyxy': boxes_xyxy[idx].astype(int),\n",
    "            'conf': float(confidences[idx]),\n",
    "            'class_id': int(class_ids[idx])\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_yolo_onnx(session, image_bgr: np.ndarray, conf_threshold: float = 0.25) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Run YOLO inference using ONNX runtime.\n",
    "    \n",
    "    Args:\n",
    "        session: ONNX InferenceSession\n",
    "        image_bgr: Input image in BGR format\n",
    "        conf_threshold: Confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        List of detections with 'bbox_xyxy' and 'conf'\n",
    "    \"\"\"\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    input_shape = session.get_inputs()[0].shape\n",
    "    # Handle dynamic dimensions (strings like 'height', 'width') - default to 640\n",
    "    h = input_shape[2] if isinstance(input_shape[2], int) else 640\n",
    "    w = input_shape[3] if isinstance(input_shape[3], int) else 640\n",
    "    target_size = (h, w)\n",
    "    \n",
    "    # Preprocess\n",
    "    img_tensor, scale, padding = preprocess_image_for_yolo(image_bgr, target_size)\n",
    "    \n",
    "    # Inference\n",
    "    outputs = session.run(None, {input_name: img_tensor})\n",
    "    \n",
    "    # Postprocess\n",
    "    detections = postprocess_yolo_output(\n",
    "        outputs[0],\n",
    "        original_shape=image_bgr.shape[:2],\n",
    "        scale=scale,\n",
    "        padding=padding,\n",
    "        conf_threshold=conf_threshold\n",
    "    )\n",
    "    \n",
    "    return detections\n",
    "\n",
    "\n",
    "print(\"ONNX YOLO helpers loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9902f596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
